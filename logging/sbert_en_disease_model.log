2020-08-13 11:49:04,336 - PyTorch version 1.4.0 available.
2020-08-13 11:49:04,428 - Load pretrained SentenceTransformer: bert-base-nli-mean-tokens
2020-08-13 11:49:04,429 - Did not find a '/' or '\' in the name. Assume to download model from server.
2020-08-13 11:49:04,442 - Load SentenceTransformer from folder: /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip
2020-08-13 11:49:04,483 - loading configuration file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/config.json
2020-08-13 11:49:04,484 - Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-08-13 11:49:04,484 - loading weights file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/pytorch_model.bin
2020-08-13 11:49:07,421 - Model name '/home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-08-13 11:49:07,422 - Didn't find file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.
2020-08-13 11:49:07,422 - loading file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/vocab.txt
2020-08-13 11:49:07,422 - loading file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/added_tokens.json
2020-08-13 11:49:07,422 - loading file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/special_tokens_map.json
2020-08-13 11:49:07,422 - loading file None
2020-08-13 11:49:07,538 - Use pytorch device: cuda
2020-08-13 11:54:47,331 - Load pretrained SentenceTransformer: bert-base-nli-mean-tokens
2020-08-13 11:54:47,332 - Did not find a '/' or '\' in the name. Assume to download model from server.
2020-08-13 11:54:47,332 - Load SentenceTransformer from folder: /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip
2020-08-13 11:54:47,332 - loading configuration file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/config.json
2020-08-13 11:54:47,333 - Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-08-13 11:54:47,333 - loading weights file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/pytorch_model.bin
2020-08-13 11:54:49,757 - Model name '/home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-08-13 11:54:49,757 - Didn't find file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.
2020-08-13 11:54:49,810 - loading file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/vocab.txt
2020-08-13 11:54:49,810 - loading file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/added_tokens.json
2020-08-13 11:54:49,811 - loading file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/special_tokens_map.json
2020-08-13 11:54:49,811 - loading file None
2020-08-13 11:54:49,890 - Use pytorch device: cuda
2020-08-13 11:55:39,192 - Load pretrained SentenceTransformer: bert-base-nli-mean-tokens
2020-08-13 11:55:39,192 - Did not find a '/' or '\' in the name. Assume to download model from server.
2020-08-13 11:55:39,192 - Load SentenceTransformer from folder: /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip
2020-08-13 11:55:39,193 - loading configuration file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/config.json
2020-08-13 11:55:39,193 - Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2020-08-13 11:55:39,193 - loading weights file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/pytorch_model.bin
2020-08-13 11:55:41,663 - Model name '/home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.
2020-08-13 11:55:41,664 - Didn't find file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.
2020-08-13 11:55:41,664 - loading file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/vocab.txt
2020-08-13 11:55:41,664 - loading file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/added_tokens.json
2020-08-13 11:55:41,664 - loading file /home/junliu/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/special_tokens_map.json
2020-08-13 11:55:41,664 - loading file None
2020-08-13 11:55:41,720 - Use pytorch device: cuda
2020-08-13 11:57:09,796 - ***************model Info***************
2020-08-13 11:57:09,796 - model name: ../models/sbert_en_disease_model.pt
2020-08-13 11:57:09,796 - epoches: 20
2020-08-13 11:57:09,796 - hidden dimensions: 128
2020-08-13 11:57:09,796 - layers: 1
2020-08-13 11:57:09,796 - dropout: 0.25
2020-08-13 11:57:09,796 - batch size: 5
2020-08-13 11:57:09,796 - ****************************************
2020-08-13 11:57:11,390 - The model has 926,748 trainable parameters
2020-08-13 11:57:43,228 - Epoch: 01 | Epoch Time: 0m 31s
2020-08-13 11:57:43,229 - 	Train Loss: 1.913 | Train micro-f1: 44.34% | Train mAP: 42.02%
2020-08-13 11:57:43,229 - 	Val.  Loss: 1.682 | Val.  micro-f1: 49.90% | Val.  mAP: 49.71%
2020-08-13 11:58:10,580 - Epoch: 02 | Epoch Time: 0m 27s
2020-08-13 11:58:10,581 - 	Train Loss: 1.551 | Train micro-f1: 52.69% | Train mAP: 52.63%
2020-08-13 11:58:10,581 - 	Val.  Loss: 1.609 | Val.  micro-f1: 52.59% | Val.  mAP: 53.95%
2020-08-13 11:58:42,846 - Epoch: 03 | Epoch Time: 0m 32s
2020-08-13 11:58:42,846 - 	Train Loss: 1.439 | Train micro-f1: 55.48% | Train mAP: 55.06%
2020-08-13 11:58:42,846 - 	Val.  Loss: 1.511 | Val.  micro-f1: 54.62% | Val.  mAP: 56.87%
2020-08-13 11:59:15,238 - Epoch: 04 | Epoch Time: 0m 32s
2020-08-13 11:59:15,238 - 	Train Loss: 1.337 | Train micro-f1: 58.26% | Train mAP: 57.57%
2020-08-13 11:59:15,238 - 	Val.  Loss: 1.527 | Val.  micro-f1: 53.57% | Val.  mAP: 57.48%
2020-08-13 11:59:48,037 - Epoch: 05 | Epoch Time: 0m 32s
2020-08-13 11:59:48,037 - 	Train Loss: 1.272 | Train micro-f1: 60.06% | Train mAP: 59.11%
2020-08-13 11:59:48,037 - 	Val.  Loss: 1.510 | Val.  micro-f1: 54.44% | Val.  mAP: 56.88%
2020-08-13 12:00:20,435 - Epoch: 06 | Epoch Time: 0m 32s
2020-08-13 12:00:20,436 - 	Train Loss: 1.181 | Train micro-f1: 62.93% | Train mAP: 60.97%
2020-08-13 12:00:20,436 - 	Val.  Loss: 1.473 | Val.  micro-f1: 55.86% | Val.  mAP: 58.94%
2020-08-13 12:00:52,917 - Epoch: 07 | Epoch Time: 0m 32s
2020-08-13 12:00:52,917 - 	Train Loss: 1.120 | Train micro-f1: 64.63% | Train mAP: 62.22%
2020-08-13 12:00:52,917 - 	Val.  Loss: 1.519 | Val.  micro-f1: 55.09% | Val.  mAP: 58.09%
2020-08-13 12:01:25,628 - Epoch: 08 | Epoch Time: 0m 32s
2020-08-13 12:01:25,629 - 	Train Loss: 1.049 | Train micro-f1: 66.72% | Train mAP: 63.50%
2020-08-13 12:01:25,629 - 	Val.  Loss: 1.476 | Val.  micro-f1: 56.90% | Val.  mAP: 59.41%
2020-08-13 12:01:57,699 - Epoch: 09 | Epoch Time: 0m 32s
2020-08-13 12:01:57,699 - 	Train Loss: 0.986 | Train micro-f1: 68.57% | Train mAP: 64.88%
2020-08-13 12:01:57,699 - 	Val.  Loss: 1.516 | Val.  micro-f1: 55.38% | Val.  mAP: 59.59%
2020-08-13 12:02:29,863 - Epoch: 10 | Epoch Time: 0m 32s
2020-08-13 12:02:29,863 - 	Train Loss: 0.915 | Train micro-f1: 71.14% | Train mAP: 66.50%
2020-08-13 12:02:29,863 - 	Val.  Loss: 1.492 | Val.  micro-f1: 57.28% | Val.  mAP: 60.04%
2020-08-13 12:03:02,138 - Epoch: 11 | Epoch Time: 0m 32s
2020-08-13 12:03:02,138 - 	Train Loss: 0.865 | Train micro-f1: 72.69% | Train mAP: 67.88%
2020-08-13 12:03:02,138 - 	Val.  Loss: 1.512 | Val.  micro-f1: 57.99% | Val.  mAP: 58.58%
2020-08-13 12:03:31,908 - Epoch: 12 | Epoch Time: 0m 29s
2020-08-13 12:03:31,908 - 	Train Loss: 0.799 | Train micro-f1: 74.62% | Train mAP: 68.76%
2020-08-13 12:03:31,908 - 	Val.  Loss: 1.521 | Val.  micro-f1: 56.48% | Val.  mAP: 59.18%
2020-08-13 12:03:55,291 - Epoch: 13 | Epoch Time: 0m 23s
2020-08-13 12:03:55,291 - 	Train Loss: 0.733 | Train micro-f1: 76.93% | Train mAP: 70.33%
2020-08-13 12:03:55,291 - 	Val.  Loss: 1.523 | Val.  micro-f1: 56.43% | Val.  mAP: 60.10%
2020-08-13 12:04:19,158 - Epoch: 14 | Epoch Time: 0m 23s
2020-08-13 12:04:19,158 - 	Train Loss: 0.679 | Train micro-f1: 78.62% | Train mAP: 71.54%
2020-08-13 12:04:19,158 - 	Val.  Loss: 1.584 | Val.  micro-f1: 56.74% | Val.  mAP: 59.09%
2020-08-13 12:04:42,719 - Epoch: 15 | Epoch Time: 0m 23s
2020-08-13 12:04:42,719 - 	Train Loss: 0.627 | Train micro-f1: 80.33% | Train mAP: 73.26%
2020-08-13 12:04:42,719 - 	Val.  Loss: 1.603 | Val.  micro-f1: 56.12% | Val.  mAP: 59.26%
2020-08-13 12:05:11,875 - Epoch: 16 | Epoch Time: 0m 29s
2020-08-13 12:05:11,876 - 	Train Loss: 0.567 | Train micro-f1: 82.24% | Train mAP: 74.56%
2020-08-13 12:05:11,876 - 	Val.  Loss: 1.654 | Val.  micro-f1: 56.95% | Val.  mAP: 60.17%
2020-08-13 12:05:44,472 - Epoch: 17 | Epoch Time: 0m 32s
2020-08-13 12:05:44,473 - 	Train Loss: 0.527 | Train micro-f1: 83.36% | Train mAP: 75.58%
2020-08-13 12:05:44,473 - 	Val.  Loss: 1.668 | Val.  micro-f1: 56.24% | Val.  mAP: 59.47%
2020-08-13 12:06:17,345 - Epoch: 18 | Epoch Time: 0m 32s
2020-08-13 12:06:17,346 - 	Train Loss: 0.499 | Train micro-f1: 84.21% | Train mAP: 76.91%
2020-08-13 12:06:17,346 - 	Val.  Loss: 1.761 | Val.  micro-f1: 54.45% | Val.  mAP: 59.34%
2020-08-13 12:06:49,915 - Epoch: 19 | Epoch Time: 0m 32s
2020-08-13 12:06:49,915 - 	Train Loss: 0.470 | Train micro-f1: 85.08% | Train mAP: 77.21%
2020-08-13 12:06:49,915 - 	Val.  Loss: 1.683 | Val.  micro-f1: 56.22% | Val.  mAP: 59.56%
2020-08-13 12:07:22,535 - Epoch: 20 | Epoch Time: 0m 32s
2020-08-13 12:07:22,535 - 	Train Loss: 0.432 | Train micro-f1: 86.27% | Train mAP: 78.59%
2020-08-13 12:07:22,535 - 	Val.  Loss: 1.837 | Val.  micro-f1: 55.05% | Val.  mAP: 58.17%
2020-08-13 12:07:29,597 - ***************Test Set Result***************
2020-08-13 12:07:29,597 - 	Val.  Loss: 1.440 | Val.  micro-f1: 56.11% | Val.  mAP: 58.95%
2020-08-13 12:07:29,597 - *********************************************
